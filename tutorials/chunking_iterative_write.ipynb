{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />    \n",
    "<br />\n",
    "<br />\n",
    "# Results of chunking and compression mechanisms\n",
    "<br />    \n",
    "            \n",
    "            \n",
    "Using compression in HDF5 requires chunking. Chunking is the process of storing different subsets of the dataset contiguously on disk. For example for an array of dimensions (36, 257, 167, 84), chunking with (18, 257, 167, 84) gives four chunks. Chunking can be utilized when anticipating reading subsets of data. Compared to storing each row of an array conitguously on disk, reading subset of the array that match the chunking mechanism at the time of storage increases efficiency.\n",
    "\n",
    "There are multiple variable that can control how the compression is done:\n",
    "\n",
    "* chunks: The shape of the chunk\n",
    "* compression: The compression algorithm, it can be either of gzip, szip, lzf, or blosc compressors such as zstd, blosclz, lz4, lz4hc, zlib\n",
    "* compression_opts: The options for each compression algorithm\n",
    "* shuffle: rearranging bytes in the data for possibility of improved compression\n",
    "\n",
    "Compression and write speed:\n",
    "\n",
    "The results in this section are dependent on the hardware. We start with considering only the Y channel. The analysis is similar for both channels.\n",
    "\n",
    "Let's consider the following chunk shape, with 'gzip', at compresion option level 3, and with shuffle turned off:\n",
    "\n",
    "|Chunk Size | Compression | Compression Options | Shuffle | Channel | File Size (MB) | Write Time (sec) | Read Time (sec) |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| (18, 36, 10, 21) |\tgzip |\t3|\t0|\tY|\t419.16|\t45.48|\t6.72|\n",
    "\n",
    "\n",
    "After compressing the Y channel only, the file size is 419.16 MB Bytes and it takes 45.48 seconds for the data to be written, and 6.72 seconds to read the data back. We try turning the shuffling on:\n",
    "\n",
    "|Chunk Size | Compression | Compression Options | Shuffle | Channel | File Size (MB) | Write Time (sec) | Read Time (sec) |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "|(18, 36, 10, 21)|\tgzip|\t3|\t1|\tY|\t380.97 | 34.66 |\t4.93 |\n",
    "\n",
    "The shuffling algorithms takes advantage of the fact that numerical data in nearby voxels have reasonably close values. It reorders the bytes representing the values of nearby voxels, and places the zeros together. This allows for better compression.\n",
    "\n",
    "Further testing shows that shuffling consistently gives significantly better results for other chunking sizes and algorithms. We will keep the shuffling on.\n",
    "\n",
    "Testing with different settings with szip shows consistent lower compression ratio and longer write time. Among blosc algorithms, zstd and lz4 performed best on the available data. gzip and zstd give the best compression size. However, gzip has significantly slower read and write times compared to blosc algorithms and lzf. \n",
    "\n",
    "\n",
    "The following shows the results for different chunking sizes.\n",
    "\n",
    "<img src=\"../media/Graph.png\" width=\"90%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up data for Benchmarking:\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "\n",
    "\n",
    "path_to_files = 'Folder/AxelLab/data'\n",
    "# info file name in path_to_files folder\n",
    "fname0 = 'fly2_run1_info.mat'\n",
    "# .mat file containing Calcium Imaging data in path_to_files folder\n",
    "fname1 = '2019_04_18_Nsyb_NLS6s_Su_walk_G_fly2_run1_8401reg.mat'\n",
    "\n",
    "\n",
    "# Open info file\n",
    "fpath0 = os.path.join(path_to_files, fname0)\n",
    "f_info = scipy.io.loadmat(fpath0, struct_as_record=False, squeeze_me=True)\n",
    "info = f_info['info']\n",
    "# Open .mat file containing Calcium Imaging data\n",
    "fpath1 = os.path.join(path_to_files, fname1)\n",
    "file = h5py.File(fpath1, 'r')\n",
    "options = file['options']\n",
    "landmarkThreshold = file['landmarkThreshold']\n",
    "templates = file['templates']\n",
    "\n",
    "Y = file['Y'] \n",
    "R = file['R']\n",
    "# Note: Changing axis order copies Y and R into memory\n",
    "Y = np.moveaxis(Y, 1, 2) \n",
    "R = np.moveaxis(R, 1, 2)\n",
    "data_shape = Y.shape\n",
    "\n",
    "# Convert back to float32\n",
    "Y = np.array(Y, dtype=np.float32)\n",
    "R = np.array(R, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up compression scenarios:\n",
    "\n",
    "# The Cartesian product of the following parameters\n",
    "# will be passed for compression\n",
    "\n",
    "# blosc compressors: 'zstd', 'blosclz','lz4','lz4hc','zlib'\n",
    "# blosc compression option range: integers 1-9 \n",
    "compression_opts_list = { 'lzf' : [ None],            # lzf does not take compression options\n",
    "                          'gzip': [ 1, 3, 6 ] }       # gzip compression option range: integers 1-9\n",
    "shuffle_list = [1] # subset of [ hdf5plugin.Blosc.NOSHUFFLE, hdf5plugin.Blosc.SHUFFLE, hdf5plugin.Blosc.BITSHUFFLE ]\n",
    "                   # or [ 0, 1, 2]\n",
    "\n",
    "chunks_list =  [(4, 20, 10, 10),\n",
    "                (4, 29, 43, 7),\n",
    "                (36, 36, 10, 6),\n",
    "                (4, 36, 11, 21)]\n",
    "\n",
    "channels = { 'Channel_Y':  Y } # subset of ( Y , R ), format { 'Channel Name': Channel_Data }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to run benchmark tests\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from pynwb import NWBFile, NWBHDF5IO, ProcessingModule\n",
    "from pynwb.ophys import TwoPhotonSeries, OpticalChannel, ImageSegmentation, Fluorescence, DfOverF, MotionCorrection\n",
    "from pynwb.device import Device\n",
    "from pynwb.base import TimeSeries\n",
    "from hdmf.backends.hdf5 import H5DataIO\n",
    "from IPython.display import clear_output, Markdown, update_display\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import functools\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import ipyvolume.pylab as p3\n",
    "from ipyvolume import ipv\n",
    "from nwbwidgets.utils.cmaps import linear_transfer_function\n",
    "from axel_lab_to_nwb import SparseIterator\n",
    "from axel_lab_to_nwb import plot_grid_seq\n",
    "\n",
    "\n",
    "def run_benchmark_tests( compression_param_product, channels, info, uncompressed_results = None, show_results = True ):\n",
    "\n",
    "    header_str = (\"Chunk Size\",\"Compression\",\"Compression Options\",\n",
    "                  \"Shuffle\",\"Write Time (sec)\",\"Read Time (sec)\",\n",
    "                  \"File Size (MB)\")\n",
    "    if uncompressed_results is not None:\n",
    "        header_str +=  (\"Compressed / Uncompressed Write Time Ratio\",\n",
    "                        \"Compressed / Uncompressed Read Time Ratio\",\n",
    "                        \"Compressed / Uncompressed Size Ratio\")\n",
    "    header_str += (\"Written / Total Chunks Ratio\", )\n",
    "    display_style = {h: \"{:.2f}\" for h in header_str[4:]}\n",
    "\n",
    "    data_frame = pd.DataFrame(columns = header_str)\n",
    "    results_file = 'results.tsv'\n",
    "\n",
    "    initial_display = True; chunks_quantized = 0\n",
    "    compression_param_product_withpbar = tqdm(list(compression_param_product))\n",
    "    for compression_params in compression_param_product_withpbar:\n",
    "\n",
    "        # progressbar update\n",
    "        compression_param_product_withpbar.set_description(\"Processing %s\" % str(compression_params) )\n",
    "        compression_param_product_withpbar.refresh()\n",
    "\n",
    "        # unpacking compression parameters\n",
    "        compression_opts, compression, chunk_shape, shuffle = *compression_params,\n",
    "        \n",
    "        run_nwb_result = run_nwb(compression_opts, compression, chunk_shape, shuffle, channels, info)\n",
    "        time_write, time_read, file_size, chunk_ratio, chunks_written, chunk_maxvalues = run_nwb_result\n",
    "\n",
    "        # prepare benchmark results\n",
    "        output_list =  [str(chunk_shape),\n",
    "                        compression,\n",
    "                        str(compression_opts),\n",
    "                        str(shuffle),\n",
    "                        time_write,\n",
    "                        time_read,\n",
    "                        file_size]\n",
    "\n",
    "        # add comp/uncompressed ratios\n",
    "        if uncompressed_results is not None:\n",
    "            output_ratio = np.array([time_write, time_read, file_size])/uncompressed_results\n",
    "            output_list += list(output_ratio)\n",
    "\n",
    "        # add ratio of written chunks\n",
    "        output_list += [chunk_ratio]\n",
    "\n",
    "        # add results to data frame\n",
    "        data_frame.loc[len(data_frame)] = output_list\n",
    "\n",
    "        if show_results == True:\n",
    "            \n",
    "            if initial_display == True:\n",
    "                clear_output()\n",
    "                p3.clear()\n",
    "                p3.figure(width = 800, controls = False)\n",
    "                ipv.style.use('seaborn-whitegrid')\n",
    "                ipv.style.box_off()\n",
    "                p3.display(p3.gcc(),display_id=\"data selection\")\n",
    "                \n",
    "                chunks_index_shape = np.ceil( np.divide( data_shape, chunk_shape ) ).astype(int)\n",
    "                chunks_index_boolean = np.zeros( chunks_index_shape, dtype = bool )\n",
    "                chunks_index_boolean[tuple(chunks_written.T)] = True\n",
    "                chunks_boolean_max = np.max( chunks_index_boolean, axis = 0)\n",
    "                chunks_quantized_bool = np.kron( chunks_boolean_max, np.ones( chunk_shape[1:], dtype = bool ) )\n",
    "\n",
    "                checkered_grid = functools.reduce( lambda x,y : np.logical_xor(x%2,y%2),\n",
    "                                                  np.ogrid[0:chunks_index_shape[1],\n",
    "                                                  0:chunks_index_shape[2],\n",
    "                                                  0:chunks_index_shape[3] ] )\n",
    "                checkered_grid_quantized = np.kron( checkered_grid, np.ones( chunk_shape[1:] ) )\n",
    "                chunks_quantized_bool_a = checkered_grid_quantized*chunks_quantized_bool\n",
    "                chunks_quantized_bool_b = np.logical_not(checkered_grid_quantized)*chunks_quantized_bool\n",
    "\n",
    "                if 'Channel_Y' in channels:\n",
    "                    p3.volshow(np.max(channels['Channel_Y'], axis = 0), controls = False,\n",
    "                               tf=linear_transfer_function([0.6, 0.6, 0.6], max_opacity=0.1))\n",
    "                if 'Channel_R' in channels:\n",
    "                    p3.volshow(np.max(channels['Channel_R'], axis = 0), controls = False,\n",
    "                               tf=linear_transfer_function([0.6, 0.6, 0.6], max_opacity=0.1))\n",
    "                p3.volshow( chunks_quantized_bool_a, controls = False,\n",
    "                            tf=linear_transfer_function([0.45,0.45, 1], max_opacity=0.75),\n",
    "                            specular_exponent=5, lighting = True) \n",
    "                p3.volshow( chunks_quantized_bool_b, controls = False,\n",
    "                            tf=linear_transfer_function([0.85,0.75, 0.6], max_opacity=0.75),\n",
    "                            specular_exponent=5, lighting = True) \n",
    "                update_display(p3.current,display_id=\"data selection\")\n",
    "                    \n",
    "                display(Markdown(''), display_id=\"figure\")\n",
    "                display(Markdown(''), display_id=\"data frame\")\n",
    "                initial_display = False\n",
    "                \n",
    "            update_display(data_frame.style.format(display_style).set_properties(width='110px'),\n",
    "                           display_id=\"data frame\")            \n",
    "\n",
    "            # store results\n",
    "            with open(results_file, 'a') as output_file:\n",
    "                output_line = '\\t'.join( str(el) for el in output_list)+'\\n' \n",
    "                output_file.write( output_line )\n",
    "            # plot results \n",
    "            if uncompressed_results is None:\n",
    "                columns = header_str[4:7]\n",
    "            else:\n",
    "                columns = header_str[7:10]\n",
    "            marker_list = [\"v\",\"d\",\"o\",\"X\"]\n",
    "            output_data_frame = data_frame.copy(deep=True) # for plotting\n",
    "            sns_plot = plot_grid_seq(output_data_frame,columns = columns,\n",
    "                        legend_columns=[\"Compression\",\"Compression Options\"],\n",
    "                        markers = marker_list, add_gridline = [1,1] )\n",
    "            \n",
    "            if sns_plot != None:\n",
    "                update_display(sns_plot.fig, display_id=\"figure\")\n",
    "            \n",
    "    return chunks_quantized#data_frame\n",
    "\n",
    "# Collect benchmark data for a given compression scenario\n",
    "# time of writing and reading data, and file size\n",
    "def run_nwb(compression_opts, compression, chunks, shuffle, channels, info):\n",
    "\n",
    "    # unpack data\n",
    "    if 'Channel_Y' in channels:\n",
    "        Y = channels['Channel_Y']\n",
    "    if 'Channel_R' in channels:\n",
    "        R = channels['Channel_R']\n",
    "    \n",
    "    #Create new NWB file\n",
    "    nwb = NWBFile(session_description='my CaIm recording', \n",
    "                  identifier='EXAMPLE_ID',\n",
    "                  session_start_time=datetime.now(tzlocal()),\n",
    "                  experimenter='Evan Schaffer',\n",
    "                  lab='Axel lab',\n",
    "                  institution='Columbia University',\n",
    "                  experiment_description='EXPERIMENT_DESCRIPTION',\n",
    "                  session_id='IDX')\n",
    "\n",
    "    #Create and add device\n",
    "    device = Device('Device')\n",
    "    nwb.add_device(device)\n",
    "\n",
    "    # Create an Imaging Plane for Yellow\n",
    "    optical_channel_Y = OpticalChannel(name='OpticalChannel_Y',\n",
    "                                       description='2P Optical Channel',\n",
    "                                       emission_lambda=510.)\n",
    "    imaging_plane_Y = nwb.create_imaging_plane(name='ImagingPlane_Y',\n",
    "                                               optical_channel=optical_channel_Y,\n",
    "                                               description='Imaging plane',\n",
    "                                               device=device,\n",
    "                                               excitation_lambda=488., \n",
    "                                               imaging_rate=info.daq.scanRate,\n",
    "                                               indicator='NLS-GCaMP6s',\n",
    "                                               location='whole central brain')\n",
    "    # Create an Imaging Plane for Red\n",
    "    optical_channel_R = OpticalChannel(name='OpticalChannel_R',\n",
    "                                       description='2P Optical Channel',\n",
    "                                       emission_lambda=633.)\n",
    "    imaging_plane_R = nwb.create_imaging_plane(name='ImagingPlane_R',\n",
    "                                               optical_channel=optical_channel_R,\n",
    "                                               description='Imaging plane',\n",
    "                                               device=device,\n",
    "                                               excitation_lambda=488., \n",
    "                                               imaging_rate=info.daq.scanRate,\n",
    "                                               indicator='redStinger',\n",
    "                                               location='whole central brain')\n",
    "\n",
    "    # output file name\n",
    "    fname_nwb = 'file_compressed.nwb'\n",
    "    output_path_to_files = 'Folder/AxelLab/data'\n",
    "    fpath_nwb = os.path.join(output_path_to_files, fname_nwb)\n",
    "    if os.path.isfile(fpath_nwb):\n",
    "        os.remove(fpath_nwb)\n",
    "\n",
    "    # compression keywords to pass to h5py\n",
    "    shuffle = bool(shuffle)\n",
    "    if compression in ['zstd','blosclz','lz4','lz4hc','zlib']:\n",
    "        compression_kw = hdf5plugin.Blosc(cname=compression, clevel=compression_opts, shuffle=shuffle)\n",
    "    else:\n",
    "        compression_kw = { 'compression' : compression, 'compression_opts' : compression_opts,\n",
    "                           'shuffle' : shuffle }\n",
    "       \n",
    "    chunk_ratio = {}\n",
    "    chunk_index_array = {}\n",
    "    chunk_maxvalues = {}    \n",
    "    if 'Channel_Y' in channels:\n",
    "        if chunks != None:\n",
    "            Y_chunk_iterator = SparseIterator(data=Y,\n",
    "                                chunk_shape=chunks)\n",
    "            chunk_ratio['Channel_Y'] = Y_chunk_iterator.chunk_ratio\n",
    "            chunk_index_array['Channel_Y'] = Y_chunk_iterator.chunk_index_array\n",
    "            chunk_maxvalues['Channel_Y'] = Y_chunk_iterator.chunk_maxvalues            \n",
    "            Y_dataio = H5DataIO(Y_chunk_iterator, chunks=chunks, fillvalue=np.nan,\n",
    "                                maxshape = (None,*Y.shape[1:]), **compression_kw)\n",
    "        else:\n",
    "            Y_dataio = H5DataIO(Y, **compression_kw)\n",
    "            chunk_ratio['Channel_Y'] = 1\n",
    "            chunk_index_array['Channel_Y'] = \"all\"\n",
    "            chunk_maxvalues['Channel_Y'] = \"all\"\n",
    "        raw_image_series_Y = TwoPhotonSeries(name='TwoPhotonSeries_Y',\n",
    "                     imaging_plane=imaging_plane_Y,\n",
    "                     rate=info.daq.scanRate,\n",
    "                     dimension=Y_dataio.shape,\n",
    "                     unit=\"unit\",\n",
    "                     data=Y_dataio)\n",
    "        nwb.add_acquisition(raw_image_series_Y)\n",
    "\n",
    "    if 'Channel_R' in channels:\n",
    "        if chunks != None:\n",
    "            R_chunk_iterator = SparseIterator(data=R,\n",
    "                                chunk_shape=chunks)\n",
    "            chunk_ratio['Channel_R'] = R_chunk_iterator.chunk_ratio\n",
    "            chunk_index_array['Channel_R'] = R_chunk_iterator.chunk_index_array\n",
    "            chunk_maxvalues['Channel_R'] = R_chunk_iterator.chunk_maxvalues                        \n",
    "            R_dataio = H5DataIO(R_chunk_iterator, chunks=chunks, fillvalue=np.nan,\n",
    "                                maxshape = (None,*R.shape[1:]), **compression_kw)\n",
    "        else:\n",
    "            R_dataio = H5DataIO(R, **compression_kw)\n",
    "            chunk_ratio['Channel_R'] = 1\n",
    "            chunk_index_array['Channel_R'] = \"all\"\n",
    "            chunk_maxvalues['Channel_R'] = \"all\"            \n",
    "        raw_image_series_R = TwoPhotonSeries(name='TwoPhotonSeries_R',\n",
    "                     imaging_plane=imaging_plane_R,\n",
    "                     rate=info.daq.scanRate,\n",
    "                     dimension=R_dataio.shape,\n",
    "                     unit=\"unit\",\n",
    "                     data=R_dataio)\n",
    "        nwb.add_acquisition(raw_image_series_R)\n",
    "\n",
    "    # start compression write clock\n",
    "    time_write_start = time.clock()        \n",
    "        \n",
    "    #Saves to NWB file\n",
    "    with NWBHDF5IO(fpath_nwb, mode='w') as io:\n",
    "        io.write(nwb)\n",
    "\n",
    "    time_write = time.clock() - time_write_start\n",
    "\n",
    "    # clear file buffer\n",
    "    if os.name == 'posix':\n",
    "        try:\n",
    "            with open(fpath_nwb) as fdforfadvise:\n",
    "                os.posix_fadvise(fdforfadvise.fileno(), 0, 0, os.POSIX_FADV_DONTNEED)\n",
    "                # normal unix file buffer \n",
    "                os.posix_fadvise(fdforfadvise.fileno(), 0, 0, os.POSIX_FADV_NORMAL)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #Loads NWB file\n",
    "    time_read_start = time.clock()\n",
    "\n",
    "    with NWBHDF5IO(fpath_nwb, mode='r') as io:\n",
    "        nwb = io.read()\n",
    "        if 'Y' in channels:\n",
    "            Y_series = nwb.acquisition['TwoPhotonSeries_Y']\n",
    "            # read data into memory\n",
    "            Y_series_data = Y_series.data[()]\n",
    "            del Y_series_data\n",
    "        if 'R' in channels:\n",
    "            R_series = nwb.acquisition['TwoPhotonSeries_R']\n",
    "            # read data into memory\n",
    "            R_series_data = R_series.data[()]\n",
    "            del R_series_data\n",
    "\n",
    "    time_read = time.clock() - time_read_start\n",
    "\n",
    "    nwbfile_size = os.stat(fpath_nwb).st_size/1024/1024# in MB        \n",
    "\n",
    "    # find ratio of wrritten chunks\n",
    "    chunk_ratio_total = sum(chunk_ratio.values())/len(chunk_ratio)\n",
    "    if \"all\" in chunk_index_array.values():\n",
    "        chunks_written = \"all\"\n",
    "        chunk_maxvalues = \"all\"\n",
    "    else:\n",
    "        chunks_written = np.vstack(tuple(chunk_index_array.values()))\n",
    "        # remove duplicates\n",
    "        chunks_written = np.unique( chunks_written, axis=0 )\n",
    "        chunk_maxvalues = functools.reduce( np.maximum , tuple(chunk_maxvalues.values()) )\n",
    "\n",
    "    return [time_write, time_read, nwbfile_size, chunk_ratio_total, chunks_written, chunk_maxvalues]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterator for nested loop of compression variables\n",
    "compression_param_product = itertools.chain.from_iterable(\n",
    "    itertools.product(compression_opts_list[compression_alg],\n",
    "                      [compression_alg],\n",
    "                      chunks_list,\n",
    "                      shuffle_list)\n",
    "                      for compression_alg in compression_opts_list)\n",
    "\n",
    "# adding uncompressed runs\n",
    "num_uncompr_runs = 3 # 1st run is warm up, 2 for finding the average time without compression\n",
    "no_compression_param_product = itertools.repeat( (None, None, None, None), num_uncompr_runs)\n",
    "data_frame = run_benchmark_tests( no_compression_param_product, channels, info, show_results = False )\n",
    "uncompressed_results = data_frame[[\"Write Time (sec)\",\"Read Time (sec)\", \"File Size (MB)\"]].loc[1:].mean()\n",
    "\n",
    "df = run_benchmark_tests( compression_param_product, channels, info, uncompressed_results = uncompressed_results )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
