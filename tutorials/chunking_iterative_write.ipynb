{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />    \n",
    "<br />\n",
    "<br />\n",
    "# Results of chunking and compression mechanisms\n",
    "<br />    \n",
    "            \n",
    "            \n",
    "Using compression in HDF5 requires chunking. Chunking is the process of storing different subsets of the dataset contiguously on disk. For example for an array of dimensions (36, 257, 167, 84), chunking with (18, 257, 167, 84) gives four chunks. Chunking can be utilized when anticipating reading subsets of data. Compared to storing each row of an array conitguously on disk, reading subset of the array that match the chunking mechanism at the time of storage increases efficiency.\n",
    "\n",
    "There are multiple variable that can control how the compression is done:\n",
    "\n",
    "* chunks: The shape of the chunk\n",
    "* compression: The compression algorithm, it can be either of gzip, szip, lzf, or blosc compressors such as zstd, blosclz, lz4, lz4hc, zlib\n",
    "* compression_opts: The options for each compression algorithm\n",
    "* shuffle: rearranging bytes in the data for possibility of improved compression\n",
    "\n",
    "Compression and write speed:\n",
    "\n",
    "The results in this section are dependent on the hardware. We start with considering only the Y channel. The analysis is similar for both channels.\n",
    "\n",
    "Let's consider the following chunk shape, with 'gzip', at compresion option level 3, and with shuffle turned off:\n",
    "\n",
    "|Chunk Size | Compression | Compression Options | Shuffle | Channel | File Size (MB) | Write Time (sec) | Read Time (sec) |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| (18, 36, 10, 21) |\tgzip |\t3|\t0|\tY|\t419.16|\t45.48|\t6.72|\n",
    "\n",
    "\n",
    "After compressing the Y channel only, the file size is 419.16 MB Bytes and it takes 45.48 seconds for the data to be written, and 6.72 seconds to read the data back. We try turning the shuffling on:\n",
    "\n",
    "|Chunk Size | Compression | Compression Options | Shuffle | Channel | File Size (MB) | Write Time (sec) | Read Time (sec) |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "|(18, 36, 10, 21)|\tgzip|\t3|\t1|\tY|\t380.97 | 34.66 |\t4.93 |\n",
    "\n",
    "The shuffling algorithms takes advantage of the fact that numerical data in nearby voxels have reasonably close values. It reorders the bytes representing the values of nearby voxels, and places the zeros together. This allows for better compression.\n",
    "\n",
    "Further testing shows that shuffling consistently gives significantly better results for other chunking sizes and algorithms. We will keep the shuffling on.\n",
    "\n",
    "Testing with different settings with szip shows consistent lower compression ratio and longer write time. Among blosc algorithms, zstd and lz4 performed best on the available data. gzip and zstd give the best compression size. However, gzip has significantly slower read and write times compared to blosc algorithms and lzf. \n",
    "\n",
    "\n",
    "The following shows the results for different chunking sizes.\n",
    "\n",
    "<img src=\"GraphA.png\" width=\"90%\"/>\n",
    "<img src=\"GraphB.png\" width=\"90%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up data for Benchmarking:\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import hdf5plugin\n",
    "import h5py\n",
    "\n",
    "\n",
    "path_to_files = 'Folder/AxelLab/data'\n",
    "# info file name in path_to_files folder\n",
    "fname0 = 'fly2_run1_info.mat'\n",
    "# .mat file containing Calcium Imaging data in path_to_files folder\n",
    "fname1 = '2019_04_18_Nsyb_NLS6s_Su_walk_G_fly2_run1_8401reg.mat'\n",
    "\n",
    "\n",
    "# Open info file\n",
    "fpath0 = os.path.join(path_to_files, fname0)\n",
    "f_info = scipy.io.loadmat(fpath0, struct_as_record=False, squeeze_me=True)\n",
    "info = f_info['info']\n",
    "# Open .mat file containing Calcium Imaging data\n",
    "fpath1 = os.path.join(path_to_files, fname1)\n",
    "file = h5py.File(fpath1, 'r')\n",
    "options = file['options']\n",
    "landmarkThreshold = file['landmarkThreshold']\n",
    "templates = file['templates']\n",
    "\n",
    "Y = file['Y'] \n",
    "R = file['R']\n",
    "# Note: Changing axis order copies Y and R into memory\n",
    "Y = np.moveaxis(Y, 1, 2) \n",
    "R = np.moveaxis(R, 1, 2)\n",
    "\n",
    "# Convert back to float32\n",
    "Y = np.array(Y, dtype=np.float32)\n",
    "R = np.array(R, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up compression scenarios:\n",
    "\n",
    "# The Cartesian product of the following parameters\n",
    "# will be passed for compression\n",
    "\n",
    "# blosc compressors: 'zstd', 'blosclz','lz4','lz4hc','zlib'\n",
    "# blosc compression option range: integers 1-9 \n",
    "compression_opts_list = { 'lzf' : [ None],            # lzf does not take compression options\n",
    "                          'gzip': [ 1, 3, 6 ] }       # gzip compression option range: integers 1-9\n",
    "shuffle_list = [1] # subset of [ hdf5plugin.Blosc.NOSHUFFLE, hdf5plugin.Blosc.SHUFFLE, hdf5plugin.Blosc.BITSHUFFLE ]\n",
    "                   # or [ 0, 1, 2]\n",
    "\n",
    "chunks_list =  [(10, 257, 7, 6),\n",
    "                (4, 29, 43, 7),\n",
    "                (36, 36, 10, 6),\n",
    "                (4, 36, 11, 21)]\n",
    "\n",
    "channels = [ 'Y' ] # subset of [ 'Y', 'R' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to run benchmark tests\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.tz import tzlocal\n",
    "from pynwb import NWBFile, NWBHDF5IO, ProcessingModule\n",
    "from pynwb.ophys import TwoPhotonSeries, OpticalChannel, ImageSegmentation, Fluorescence, DfOverF, MotionCorrection\n",
    "from pynwb.device import Device\n",
    "from pynwb.base import TimeSeries\n",
    "from hdmf.backends.hdf5 import H5DataIO\n",
    "from IPython.display import clear_output, Markdown, update_display\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def RunBenchmarkTests( compression_param_product, channels, info, uncompressed_results = None, show_results = True ):\n",
    "\n",
    "    header_str = (\"Chunk Size\",\"Compression\",\"Compression Options\",\n",
    "                  \"Shuffle\",\"Write Time (sec)\",\"Read Time (sec)\",\"File Size (MB)\")\n",
    "    display_style = {h: '{:.2f}'.format for h in header_str[4:7]}\n",
    "    \n",
    "    if uncompressed_results is not None:\n",
    "        header_str +=  (\"Compressed / Uncompressed Write Time %\",\n",
    "                        \"Compressed / Uncompressed Read Time %\",\n",
    "                        \"Compressed / Uncompressed Size %\")\n",
    "        display_style.update({h: '{:.2f}%'.format for h in header_str[7:10]})\n",
    "   \n",
    "    data_frame = pd.DataFrame(columns = header_str)\n",
    "    results_file = 'results.tsv'\n",
    "\n",
    "    clear_output()\n",
    "    display(Markdown('') , display_id=\"figure\")\n",
    "    display(Markdown(''), display_id=\"data frame\")\n",
    "\n",
    "    compression_param_product_withpbar = tqdm(list(compression_param_product))\n",
    "    for compression_params in compression_param_product_withpbar:\n",
    "\n",
    "        # progressbar update\n",
    "        compression_param_product_withpbar.set_description(\"Processing %s\" % str(compression_params) )\n",
    "        compression_param_product_withpbar.refresh()\n",
    "\n",
    "        # unpacking compression parameters\n",
    "        compression_opts, compression, chunks, shuffle = *compression_params,\n",
    "        \n",
    "        # run nwb with compression parameters\n",
    "        time_write, time_read, file_size = RunNwb(compression_opts, compression, chunks, shuffle, channels)\n",
    "        \n",
    "        # prepare benchmark results        \n",
    "        output_list =  [str(chunks),\n",
    "                        compression,\n",
    "                        str(compression_opts),\n",
    "                        str(shuffle),\n",
    "                        time_write,\n",
    "                        time_read,\n",
    "                        file_size]\n",
    "\n",
    "        # add comp/uncompressed ratios\n",
    "        if uncompressed_results is not None: \n",
    "            output_percentage = np.array([time_write, time_read, file_size])/uncompressed_results*100\n",
    "            output_list += list(output_percentage)\n",
    "\n",
    "        # add results to data frame\n",
    "        data_frame.loc[len(data_frame)] = output_list\n",
    "\n",
    "        if show_results == True:\n",
    "            update_display(data_frame.style.format(display_style).set_properties(width='100px'),\n",
    "                           display_id=\"data frame\")            \n",
    "\n",
    "            # store results\n",
    "            with open(results_file, 'a') as output_file:\n",
    "                output_line = '\\t'.join( str(el) for el in output_list)+'\\n' \n",
    "                output_file.write( output_line )\n",
    "            \n",
    "            # plot results \n",
    "            if uncompressed_results is None:\n",
    "                columns = header_str[4:7]\n",
    "            else:\n",
    "                columns = header_str[7:10]\n",
    "            marker_list = [\"v\",\"d\",\"o\",\"X\"]\n",
    "            output_data_frame = data_frame.copy(deep=True) # for plotting\n",
    "            PlotGridSeq(output_data_frame,columns = columns,legend_columns=[\"Compression\",\"Compression Options\"],\n",
    "                        compression_types = compression_opts_list, markers = marker_list )\n",
    "            \n",
    "    return data_frame\n",
    "\n",
    "def RunNwb(compression_opts, compression, chunks, shuffle, channels):\n",
    "\n",
    "    #Create new NWB file\n",
    "    nwb = NWBFile(session_description='my CaIm recording', \n",
    "                  identifier='EXAMPLE_ID',\n",
    "                  session_start_time=datetime.now(tzlocal()),\n",
    "                  experimenter='Evan Schaffer',\n",
    "                  lab='Axel lab',\n",
    "                  institution='Columbia University',\n",
    "                  experiment_description='EXPERIMENT_DESCRIPTION',\n",
    "                  session_id='IDX')\n",
    "\n",
    "    #Create and add device\n",
    "    device = Device('Device')\n",
    "    nwb.add_device(device)\n",
    "\n",
    "    # Create an Imaging Plane for Yellow\n",
    "    optical_channel_Y = OpticalChannel(name='OpticalChannel_Y',\n",
    "                                       description='2P Optical Channel',\n",
    "                                       emission_lambda=510.)\n",
    "    imaging_plane_Y = nwb.create_imaging_plane(name='ImagingPlane_Y',\n",
    "                                               optical_channel=optical_channel_Y,\n",
    "                                               description='Imaging plane',\n",
    "                                               device=device,\n",
    "                                               excitation_lambda=488., \n",
    "                                               imaging_rate=info.daq.scanRate,\n",
    "                                               indicator='NLS-GCaMP6s',\n",
    "                                               location='whole central brain')\n",
    "    # Create an Imaging Plane for Red\n",
    "    optical_channel_R = OpticalChannel(name='OpticalChannel_R',\n",
    "                                       description='2P Optical Channel',\n",
    "                                       emission_lambda=633.)\n",
    "    imaging_plane_R = nwb.create_imaging_plane(name='ImagingPlane_R',\n",
    "                                               optical_channel=optical_channel_R,\n",
    "                                               description='Imaging plane',\n",
    "                                               device=device,\n",
    "                                               excitation_lambda=488., \n",
    "                                               imaging_rate=info.daq.scanRate,\n",
    "                                               indicator='redStinger',\n",
    "                                               location='whole central brain')\n",
    "\n",
    "    # output file name\n",
    "    fname_nwb = 'file_compressed.nwb'\n",
    "    output_path_to_files = 'Folder/AxelLab/data'\n",
    "    fpath_nwb = os.path.join(output_path_to_files, fname_nwb)\n",
    "    if os.path.isfile(fpath_nwb):\n",
    "        os.remove(fpath_nwb)\n",
    "\n",
    "    # compression keywords to pass to h5py\n",
    "    shuffle = bool(shuffle)\n",
    "    if compression in ['zstd','blosclz','lz4','lz4hc','zlib']:\n",
    "        compression_kw = hdf5plugin.Blosc(cname=compression, clevel=compression_opts, shuffle=shuffle)\n",
    "    else:\n",
    "        compression_kw = { 'compression' : compression, 'compression_opts' : compression_opts,\n",
    "                           'shuffle' : shuffle }\n",
    "\n",
    "       \n",
    "    if 'Y' in channels:\n",
    "        if chunks != None:\n",
    "            Y_chunk_iterator = SparseMatrixIterator(data=Y,\n",
    "                                chunk_shape=chunks)\n",
    "            Y_dataio = H5DataIO(Y_chunk_iterator, chunks=chunks, fillvalue=np.nan,\n",
    "                                maxshape = (None,*Y.shape[1:]), **compression_kw)        \n",
    "        else:\n",
    "            Y_dataio = H5DataIO(Y, **compression_kw)        \n",
    "        raw_image_series_Y = TwoPhotonSeries(name='TwoPhotonSeries_Y', \n",
    "                     imaging_plane=imaging_plane_Y,\n",
    "                     rate=info.daq.scanRate,\n",
    "                     dimension=Y_dataio.shape,\n",
    "                     unit=\"unit\",\n",
    "                     data=Y_dataio)\n",
    "        nwb.add_acquisition(raw_image_series_Y)            \n",
    "\n",
    "    if 'R' in channels:\n",
    "        if chunks != None:\n",
    "            R_chunk_iterator = SparseMatrixIterator(data=R,\n",
    "                                chunk_shape=chunks)\n",
    "            \n",
    "            R_dataio = H5DataIO(R_chunk_iterator, chunks=chunks, fillvalue=np.nan,\n",
    "                                maxshape = (None,*R.shape[1:]), **compression_kw)        \n",
    "        else:\n",
    "            R_dataio = H5DataIO(R, **compression_kw)        \n",
    "            \n",
    "        raw_image_series_R = TwoPhotonSeries(name='TwoPhotonSeries_R', \n",
    "                     imaging_plane=imaging_plane_R,\n",
    "                     rate=info.daq.scanRate,\n",
    "                     dimension=R_dataio.shape,\n",
    "                     unit=\"unit\",\n",
    "                     data=R_dataio) \n",
    "        nwb.add_acquisition(raw_image_series_R)            \n",
    "\n",
    "    # start compression write clock\n",
    "    time_write_start = time.clock()        \n",
    "        \n",
    "    #Saves to NWB file\n",
    "    with NWBHDF5IO(fpath_nwb, mode='w') as io:\n",
    "        io.write(nwb)\n",
    "\n",
    "    time_write = time.clock() - time_write_start\n",
    "\n",
    "\n",
    "    # clear file buffer\n",
    "    if os.name == 'posix':\n",
    "        try:\n",
    "            with open(fpath_nwb) as fdforfadvise:\n",
    "                os.posix_fadvise(fdforfadvise.fileno(), 0, 0, os.POSIX_FADV_DONTNEED)\n",
    "                # normal unix file buffer \n",
    "                os.posix_fadvise(fdforfadvise.fileno(), 0, 0, os.POSIX_FADV_NORMAL)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    \n",
    "    #Loads NWB file\n",
    "    time_read_start = time.clock()\n",
    "\n",
    "    with NWBHDF5IO(fpath_nwb, mode='r') as io:\n",
    "        nwb = io.read()\n",
    "        if 'Y' in channels:\n",
    "            Y_series = nwb.acquisition['TwoPhotonSeries_Y']\n",
    "            # read data into memory\n",
    "            Y_series_data = Y_series.data[()]\n",
    "            del Y_series_data\n",
    "        if 'R' in channels:\n",
    "            R_series = nwb.acquisition['TwoPhotonSeries_R']\n",
    "            # read data into memory\n",
    "            R_series_data = R_series.data[()]\n",
    "            del R_series_data\n",
    "\n",
    "    time_read = time.clock() - time_read_start\n",
    "\n",
    "    nwbfile_size = os.stat(fpath_nwb).st_size/1024/1024# in MB        \n",
    "\n",
    "    return time_write, time_read, nwbfile_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select chunks to write to disk\n",
    "\n",
    "from hdmf.data_utils import AbstractDataChunkIterator, DataChunk\n",
    "from scipy.ndimage import morphology\n",
    "import functools\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import signal, stats, spatial\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "class SparseMatrixIterator(AbstractDataChunkIterator):\n",
    "\n",
    "    def __init__(self, data, chunk_shape):\n",
    "        \"\"\"\n",
    "        :param shape: 2D tuple with the shape of the matrix\n",
    "        :param chunk_shape: The shape of each chunk to be created\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.shape, self.chunk_shape = data.shape, chunk_shape\n",
    "        self.data = data\n",
    "        self.__chunks_created = 0\n",
    "        \n",
    "        self.chunk_index_list = self.BlobDetection()        \n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Return in each iteration a fully occupied data chunk of self.chunk_shape values at selected\n",
    "        location within the matrix. Chunks are non-overlapping.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.chunk_index_list) != 0:\n",
    "            chunk_index=self.chunk_index_list.pop(0)\n",
    "            tmin = chunk_index[0] * self.chunk_shape[0]\n",
    "            tmax = tmin + self.chunk_shape[0]\n",
    "            xmin = chunk_index[1] * self.chunk_shape[1]\n",
    "            xmax = xmin + self.chunk_shape[1]\n",
    "            ymin = chunk_index[2] * self.chunk_shape[2]\n",
    "            ymax = ymin + self.chunk_shape[2]\n",
    "            zmin = chunk_index[3] * self.chunk_shape[3]\n",
    "            zmax = zmin + self.chunk_shape[3]\n",
    "                        \n",
    "            selection = np.s_[tmin:tmax, xmin:xmax, ymin:ymax, zmin:zmax]\n",
    "            data = self.data[selection]\n",
    "            \n",
    "            self.__chunks_created += 1\n",
    "            return DataChunk(data=data, selection=selection)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    next = __next__\n",
    "\n",
    "    def recommended_chunk_shape(self):\n",
    "        # Here we can optionally recommend what a good chunking should be.\n",
    "        return self.chunk_shape\n",
    "\n",
    "    def recommended_data_shape(self):\n",
    "        # In cases where we don't know the full size this should be the minimum size.\n",
    "        return (1,*self.data.shape[1:])\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        # The data type of our array\n",
    "        return np.dtype(np.float32)\n",
    "\n",
    "    @property\n",
    "    def maxshape(self):\n",
    "        # If we don't know the size of a dimension beforehand we can set the dimension to None instead\n",
    "        return (None,*self.data.shape[1:])\n",
    "\n",
    "    def BlobDetection(self):\n",
    "        \"\"\"\n",
    "        :return: list of chunk indices of interest\n",
    "        \"\"\"\n",
    "\n",
    "        # construct list of chunk indices\n",
    "        chunk_index_shape = np.floor_divide( self.data.shape, self.chunk_shape )\n",
    "        chunk_index_count = np.arange(np.prod( chunk_index_shape )).reshape( chunk_index_shape )\n",
    "        chunk_index_grid = np.argwhere( chunk_index_count > -1 ).astype(int)\n",
    "        \n",
    "        # thresholding\n",
    "        _ratio = 7\n",
    "        time_len = len(self.data)\n",
    "        data_downsample = self.data[:,::_ratio,::_ratio,::_ratio]\n",
    "        num_val = np.prod(data_downsample.shape)\n",
    "        data_downsample_morph_grad = morphology.morphological_gradient(data_downsample, size=(time_len,_ratio,_ratio,_ratio ))\n",
    "        # find histogram\n",
    "        relhist = stats.relfreq(data_downsample_morph_grad,\n",
    "                                numbins = int(num_val/(8**3))+1, )\n",
    "        relhist.lowerlimit + np.linspace(0, relhist.binsize*relhist.frequency.size, relhist.frequency.size)\n",
    "        peaks, _ = signal.find_peaks(relhist.frequency, distance = 2)\n",
    "        # threashold at 70% of histogram peak\n",
    "        threshold = max(peaks)*0.7\n",
    "        border_mask = data_downsample_morph_grad > threshold\n",
    "        border_points = np.transpose(np.nonzero(border_mask))\n",
    "        border_points = border_points*np.array([1,_ratio,_ratio,_ratio])\n",
    "\n",
    "        # find chunks indices in border_points convex hull\n",
    "        hull = scipy.spatial.ConvexHull( border_points )\n",
    "        chunk_index_list = [chunk_index for chunk_index in chunk_index_grid\n",
    "                            if self.ChunkIsInHull(hull, chunk_index*self.chunk_shape ) ]\n",
    "        \n",
    "        return chunk_index_list\n",
    "    \n",
    "    def ChunkIsInHull(self, hull, point):\n",
    "        \"\"\"\n",
    "        :param hull: Convex hull object returned by scipy.spatial.ConvexHull\n",
    "        :param point: The index point representing the chunk nD-cube\n",
    "        :return: True if the chunk nd-Cube and the convex hull intersect\n",
    "        \"\"\"\n",
    "        \n",
    "        chunk_equations = np.zeros([ len(point)*2 , len(point) + 1 ])\n",
    "        for dim in range(len(point)):\n",
    "            chunk_equations[dim*2    , dim] = -1\n",
    "            chunk_equations[dim*2 + 1, dim] =  1\n",
    "            chunk_equations[dim*2    , -1]  = point[dim]\n",
    "            chunk_equations[dim*2 + 1, -1]  = point[dim]\n",
    "\n",
    "        equations = np.vstack( (hull.equations, chunk_equations) )\n",
    "        A_eq = np.transpose(equations[:,0:4])\n",
    "        b_eq = np.zeros(A_eq.shape[0])\n",
    "        A_ub = equations[:,4].reshape(1,equations.shape[0])\n",
    "        b_ub = np.array([0])\n",
    "        c = np.ones(A_eq.shape[1])\n",
    "        lp = linprog(c, A_ub, b_ub, A_eq, b_eq)\n",
    "\n",
    "        return lp.success and np.sum(lp.x) > 0 and np.sum(lp.x) < 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot function for compression results\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def PlotGridSeq(df, columns, legend_columns, compression_types, ax_lims = None, markers = None):\n",
    "\n",
    "    if ax_lims != None:\n",
    "        if len(ax_lims) != len(columns):\n",
    "            raise IndexError(\"number of ax_lims is not equal to the number of columns\")\n",
    "        # remove data not in axis limits\n",
    "        df = df[ functools.reduce(   lambda x,y: x & ( df[y] < ax_lims[y][1] ) & ( df[y] > ax_lims[y][0] ), columns, True ) ]\n",
    "\n",
    "    # add markers\n",
    "    if markers == None:\n",
    "        markers = sns.mpl.lines.Line2D.filled_markers\n",
    "\n",
    "    df_mod = pd.concat([ df[legend_columns],\n",
    "                         pd.DataFrame([{\"Compression\" : str(c), \"Compression Options\": str(el)}\n",
    "                                      for c, opts in compression_types.items() for el in opts ]) ],\n",
    "                       ignore_index = True)    \n",
    "    df_mod = df_mod.drop_duplicates()\n",
    "\n",
    "    _lightest_col = 0.4 # > 0 and < 1\n",
    "    df_mod_group_compression = df_mod.sort_values(legend_columns).groupby([\"Compression\"])    \n",
    "    palette_saturation = df_mod_group_compression.apply(lambda x: pd.Series(\n",
    "            _lightest_col + ( 1 - _lightest_col )*(np.arange(len(x))+1)/len(x) )).tolist() # saturation > 0 and < 1\n",
    "    current_palette = sns.color_palette()\n",
    "    palette_colors = df_mod_group_compression.ngroup().apply(lambda x: current_palette[x] ).tolist()\n",
    "    sns_ncolors_percompression = 10 # larger than total count of each algorithm's compression options\n",
    "    palette_order = [sns.light_palette(color = palette_colors[i],\n",
    "                       n_colors=sns_ncolors_percompression)[ int(palette_saturation[i]*sns_ncolors_percompression-1) ]\n",
    "                     for i in range(len(palette_saturation)) ]\n",
    "\n",
    "    # expand markers list if not of sufficient length\n",
    "    if len(markers) < len(df_mod_group_compression):\n",
    "        markers += tuple(set(matplotlib.lines.Line2D.filled_markers) - set(markers))\n",
    "    marker_order = df_mod_group_compression.ngroup().apply(lambda x: markers[x] ).tolist()\n",
    "\n",
    "    # format legends\n",
    "    LegendFormatFunc = lambda x: ''.join(str(i).ljust(12) for i in x)\n",
    "    legend_title = \"Compression Option\"\n",
    "    legend_title = LegendFormatFunc(legend_title.split())\n",
    "    df[legend_title] = df[legend_columns].astype(str).apply(LegendFormatFunc, axis = 1)\n",
    "    legend_order = df_mod_group_compression.apply(lambda x: pd.Series(\n",
    "            sorted(x.astype(str).apply(LegendFormatFunc, axis = 1))  )).tolist() # saturation > 0 and < 1\n",
    "    \n",
    "    # remove unused elements from legend_order, palette_order, and marker_order\n",
    "    legend_present = df[legend_title].unique()    \n",
    "    legend_order,palette_order,marker_order = map(list, zip(*[(i,j,k) for i,j,k in \n",
    "                                             zip( legend_order,palette_order,marker_order) if i in legend_present]) )\n",
    "    \n",
    "    plot_dpi = 300\n",
    "    sns.set_style(\"darkgrid\") \n",
    "    sns.set(rc={'figure.facecolor': '#F8F8F8'})\n",
    "    sns.set_context(context=\"notebook\", font_scale=1.2)\n",
    "   \n",
    "    # workaround to skip kde plots with single data point\n",
    "    single_elements = df.groupby(legend_columns).size() == 1\n",
    "    if not single_elements.any():\n",
    "        try:\n",
    "            sns_plot = sns.pairplot(vars=columns, hue = legend_title , hue_order = legend_order,\n",
    "                                    palette = palette_order, data = df,\n",
    "                                    markers= marker_order, plot_kws={ \"s\": 150, \"linewidth\": 0.05 }, height = 5 )\n",
    "            # set axis limits\n",
    "            if ax_lims != None:\n",
    "                for i in range(len(columns)):\n",
    "                    sns_plot.axes[i,i].set_xbound( ax_lims[columns[i]] )\n",
    "                    sns_plot.axes[i,i].set_ybound( ax_lims[columns[i]] )\n",
    "\n",
    "            # show plot\n",
    "            update_display(sns_plot.fig, display_id=\"figure\")\n",
    "            sns_plot.savefig(\"figure.png\", dpi=plot_dpi)\n",
    "            plt.close(sns_plot.fig)\n",
    "            \n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterator for nested loop of compression variables\n",
    "compression_param_product = itertools.chain.from_iterable(\n",
    "    itertools.product(compression_opts_list[compression_alg],\n",
    "                      [compression_alg],\n",
    "                      chunks_list,\n",
    "                      shuffle_list)\n",
    "                      for compression_alg in compression_opts_list)\n",
    "\n",
    "# adding uncompressed runs\n",
    "num_uncompr_runs = 4 # 1st run is warm up, 3 for finding the average time without compression\n",
    "no_compression_param_product = itertools.repeat( (None, None, None, None), num_uncompr_runs)\n",
    "data_frame = RunBenchmarkTests( no_compression_param_product, channels, info, show_results = False )\n",
    "uncompressed_results = data_frame.mean()\n",
    "\n",
    "df = RunBenchmarkTests( compression_param_product, channels, info, uncompressed_results = uncompressed_results )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
